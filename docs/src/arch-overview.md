# Overview

![Architecture Overview](../images/architecture-overview.png)

By default, the **CruiseKube Helm chart** deploys:

- **Controller**
  - Telemetry Engine
  - Runtime Optimizer
- **Webhook**
  - Admission Optimizer
- **Frontend**
  - Observable interface for recommendations
  - Setting User configurations per workload
  - Potential savings once CruiseKube is enabled in Cruise mode
- **Database**:
  - Stores the statistics generated by the Statistics Engine
  - Stores the user configurations per workload

# Core Components

Conceptually, CruiseKube is composed of three tightly-coupled components:

### Statistics Engine

- Continuously evaluates CPU and memory usage for each workload
- Track instances of high CPU load and memory OOMs
- Derives stable statistics (percentiles, headroom, variability)
- Persists computed metrics in an internal datastore
- Built on **Prometheus** as the primary metrics source

### Runtime Optimizer

- Implemented as a reconciliation loop in `cruisekube-controller`
- Iteratively optimizes **running workloads**, one node at a time
- Keeps the priority of individual workloads into account to minimise disruption

### Admission Optimizer

- Implemented as a **mutating admission webhook**
- Intercepts new pod creations
- Rewrites resource requests using learned recommendations

## Control Flows

### Statistics Engine

```mermaid
sequenceDiagram
  %% Autopilot Telemetry Engine – Metrics Collection & Feature Building

  participant P as Prometheus
  participant T as Statistics Engine
  participant S as Kubernetes API Server
  participant DB as Database

  loop Every scrape/aggregation interval
    T->>S: Listh Pods, Nodes (metadata)
    T->>P: Query metrics (usage, throttling, pressure)
    T->>T: Calculate stats per namespace/workload/container
    T->>T: Compute aggregates (e.g., percentiles, peaks, trends)
    T->>DB: Persist container stats
  end

```

1. Connects to target cluster prometheus and cluster
2. Calculates stats related to CPU usage, CPU pressure, memory usage, OOM instances etc
3. Stores the calculated telemetry into database

### **Runtime Optimizer Flow**

```mermaid
sequenceDiagram
  %% Autopilot Core Loop (simplified)
  participant C as Autopilot Controller
  participant M as Database
%%   participant N as Node
  participant K as Kube API

  loop Every reconcile interval
    C->>M: Read usage + pressure signals (per pod/container)
    C->>K: Read node allocatable + pod placement
    %% C->>C: Partition pods (optimizable vs reserved)
    C->>C: Estimate StableDemand + SpikeDemand (PSI-adjusted if available)
    alt Fits within node capacity
      C->>C: Distribute SpikeDemand amongst pods
      C->>K: Patch pod resources in-place
    else Not feasible
      C->>C: Choose eviction candidates by priority
      C->>K: Evict pods until feasible
      C->>C: Distribute SpikeDemand amongst pods
      C->>K: Patch pod resources in-place
    end
  end
```

1. Connect to target cluster to iterate over nodes
2. Fetch workload statistics from DB
3. Adjusts resources in-place for pods on the node

### **Admission Optimizer Flow**

```mermaid
sequenceDiagram
  %% Autopilot Admission Webhook – Scheduling-Time Optimization

  participant U as User / Controller
  participant K as Kubernetes API Server
  participant W as Autopilot Mutating Webhook
  participant M as Database
%%   participant S as Scheduler
%%   participant N as Node

  U->>K: Create Pod
  K->>W: AdmissionReview (PodSpec)
  W->>M: Fetch historical usage(for workload / container)
  W->>W: Estimate StableDemand + SpikeDemand
  W->>W: Compute initial pod requests = Stable + Spike
  W-->>K: Mutated PodSpec (updated requests/limits)
%%   K->>S: Schedule Pod
%%   S->>N: Bind Pod to Node
%%   N-->>K: Pod running

```

1. Intercept pod spec
2. Fetch statistics from the controller
3. Mutate requests before scheduling